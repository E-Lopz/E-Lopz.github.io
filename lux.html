<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LuxAi3 Agent</title>
    <!-- Styles -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <link rel="stylesheet" href="assets/style.css">
</head>
<body>
    <!-- Navbar -->
    <nav class="sticky-top">
        <a id="about-link" href="index.html">{Home}</a>
        <a id="about-link" href="index.html#projects">{More Projects}</a>
        <!-- <a id="contact-link" href="#contact">{Contact}</a> -->
        <span>Confused Human @ World</span>
        <span>Montreal, Canada </span>
    </nav>

    <div class="container">
        <div class="project-head">
            <h1 class="title">Reinforcement Learning Agent</h1>
            <h4 class="subtitle"><a href="https://www.lux-ai.org">LuxAI3 Competition</a></h4>
            <div class="content">
                <img class="project-image" src="assets/lux_main.png" alt="Cool project image">
            </div>
            <p>
                <br>I wanted to experiment with reinforcement learning and see what I could create.  
                The Lux AI Challenge was the perfect opportunity—a competition where agents face off in a 1v1 scenario, solving problems like resource gathering, allocation, and optimization.  
                It’s not just about building an agent that works; it’s about developing one that can adapt to its opponent and make smart decisions in a constantly changing environment.
            </p>
        </div>

        <!-- Methodology Section -->
        <section id="methodology">
            <h2>Methodology</h2>
            <p>
                This project implements a simplified reinforcement learning architecture where a single unit (out of a possible 16) is controlled. The agent leverages a PPO-based controller, which efficiently handles the decision-making process by mapping observations to actions. This streamlined approach allows for focused experimentation and optimizations on a smaller, manageable scale while ensuring adaptability to more complex scenarios in the future.
            </p>
        </section>

        <!-- Feature Space Section -->
        <section id="feature-space">
            <h2>Feature Space</h2>
            <p>
                To mitigate the curse of dimensionality, an observation wrapper (obswrapper) is employed to simplify the feature space. This wrapper reduces the complexity of the environment state by retaining only the most relevant features for decision-making. The selected features are carefully engineered to provide a balance between informativeness and computational efficiency, ensuring the agent's performance remains robust under varying conditions.
            </p>
        </section>

        <!-- Training and Performance Section -->
        <section id="training-performance">
            <h2>Training and Performance</h2>
            <p>
                Training is conducted using the <code>train.py</code> script, which leverages rollout workers, PPO optimization, and self-play to develop a robust agent.  
                <ul>
                    <li><strong>Rollout Workers:</strong> These parallelize environment interactions, gathering trajectories of states, actions, and rewards efficiently. This approach accelerates data collection and ensures diverse experiences for training.</li>
                    <li><strong>PPO Optimization:</strong> Proximal Policy Optimization (PPO) refines the policy network by balancing exploration and exploitation while maintaining stable learning. This ensures that updates to the policy do not deviate significantly, preserving performance consistency.</li>
                    <li><strong>Self-Play:</strong> The agent trains against itself to iteratively improve its strategies, adapting dynamically to increasingly challenging opponents. Self-play fosters the development of robust policies capable of generalizing to unseen opponents.</li>
                </ul>
                This training framework enables the agent to learn effectively, optimizing resource allocation and decision-making processes within the simplified single-unit scenario.
            </p>
        </section>

        <!-- Future Work Section -->
        <section id="future-work">
            <h2>Future Work</h2>
            <p>
                Future development goals include scaling the model to control all 16 units simultaneously, further augmenting the feature set to improve decision-making capabilities, and implementing sapping actions to expand the agent's action space. These advancements aim to enhance the agent's strategic depth and adaptability, paving the way for competitive performance in more complex environments.
            </p>
        </section>
        <section id="repository-link">
            <h2>Code</h2>
            <p>
                All the code for this project, including the training scripts, environment setup, and agent implementation, is available in the GitHub repository.  
                <a href="https://github.com/E-Lopz/LuxAi3" target="_blank">Code</a>
            </p>
        </section>
    </div>


    <!-- Footer -->
    <footer>
        <p>Last Updated: December 25, 2024</p>
    </footer>
</body>
</html>
</body>
</html>